{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e8ca1c2",
   "metadata": {},
   "source": [
    "### Remote Index Strategy\n",
    "\n",
    "This notebook always scans the full GOV.UK Employment Appeal Tribunal universe.\n",
    "\n",
    "If `remote_index.json` already exists:\n",
    "- it is treated as a cache, not a source of truth\n",
    "- previously indexed slugs are skipped\n",
    "- only new or missing decisions are fetched and HEAD-checked\n",
    "\n",
    "This design ensures:\n",
    "- crash-safe resume (no lost progress)\n",
    "- detection of newly added decisions\n",
    "- immunity to GOV.UK reordering or backfills\n",
    "\n",
    "The Search API scan is cheap; Content API fetches and PDF HEAD requests are the expensive steps and are only performed for unseen decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a00938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from urllib.parse import urljoin, urlparse, quote\n",
    "import json\n",
    "\n",
    "from eat_remote_index import (\n",
    "    HttpClient,\n",
    "    GOVUK,\n",
    "    write_json,\n",
    "    _pick_pdf_from_content_api,   # returns first PDF (fine for now)\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# SINGLE SELECTOR (HAMSTER SAFE)\n",
    "# =========================\n",
    "MODE = \"EAT\"   # <-- set to \"EAT\" or \"ET\" ONLY\n",
    "\n",
    "if MODE not in {\"EAT\", \"ET\"}:\n",
    "    raise ValueError(\"MODE must be 'EAT' or 'ET'\")\n",
    "\n",
    "CONFIG = {\n",
    "    \"EAT\": {\n",
    "        \"doc_type\": \"employment_appeal_tribunal_decision\",\n",
    "        \"base_path\": \"/employment-appeal-tribunal-decisions/\",\n",
    "        \"out_name\": \"remote_index_eat.json\",\n",
    "        \"tmp_name\": \"remote_index_eat.tmp.json\",\n",
    "    },\n",
    "    \"ET\": {\n",
    "        \"doc_type\": \"employment_tribunal_decision\",\n",
    "        \"base_path\": \"/employment-tribunal-decisions/\",\n",
    "        \"out_name\": \"remote_index_et.json\",\n",
    "        \"tmp_name\": \"remote_index_et.tmp.json\",\n",
    "    },\n",
    "}[MODE]\n",
    "\n",
    "# =========================\n",
    "# MANIFEST PATHS (HARD, EXPLICIT)\n",
    "# =========================\n",
    "BASE_MANIFESTS = Path(\"/home/hello/Appeal_reader/manifests\").resolve()\n",
    "OUT_DIR = (BASE_MANIFESTS / MODE).resolve()\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_PATH = OUT_DIR / CONFIG[\"out_name\"]\n",
    "TMP_PATH = OUT_DIR / CONFIG[\"tmp_name\"]\n",
    "\n",
    "CHECKPOINT_EVERY = 200\n",
    "MAX_ITEMS = None  # set to 200 for smoke test\n",
    "\n",
    "client = HttpClient(timeout=30.0, max_retries=4, backoff_base=0.8, min_delay=0.15)\n",
    "\n",
    "def atomic_checkpoint(obj, out_path: Path, tmp_path: Path):\n",
    "    write_json(tmp_path, obj)\n",
    "    tmp_path.replace(out_path)\n",
    "\n",
    "def slug_from_url(url: str, base_path: str) -> str:\n",
    "    path = urlparse(url).path.rstrip(\"/\")\n",
    "    return path.split(base_path)[-1].strip(\"/\")\n",
    "\n",
    "def iter_search_results_with_tqdm_local(client, *, doc_type: str, count: int = 200, max_items=None, order: str = \"-public_timestamp\"):\n",
    "    \"\"\"\n",
    "    GOV.UK Search API iterator with tqdm that initializes after first response\n",
    "    (so we know 'total').\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from tqdm.auto import tqdm\n",
    "    except Exception:\n",
    "        def tqdm(x, **kwargs):\n",
    "            return x\n",
    "\n",
    "    start = 0\n",
    "    yielded = 0\n",
    "    pbar = None\n",
    "\n",
    "    while True:\n",
    "        url = (\n",
    "            f\"{GOVUK}/api/search.json\"\n",
    "            f\"?filter_document_type={quote(doc_type)}\"\n",
    "            f\"&order={quote(order)}\"\n",
    "            f\"&count={count}\"\n",
    "            f\"&start={start}\"\n",
    "        )\n",
    "        payload = client.get_json(url)\n",
    "        results = payload.get(\"results\") or []\n",
    "        if not results:\n",
    "            break\n",
    "\n",
    "        if pbar is None:\n",
    "            total = payload.get(\"total\")\n",
    "            pbar = tqdm(total=total, desc=f\"{MODE} remote index\", unit=\"doc\")\n",
    "\n",
    "        for r in results:\n",
    "            yield r\n",
    "            yielded += 1\n",
    "            pbar.update(1)\n",
    "            if max_items is not None and yielded >= max_items:\n",
    "                pbar.close()\n",
    "                return\n",
    "\n",
    "        start += len(results)\n",
    "        total = payload.get(\"total\")\n",
    "        if isinstance(total, int) and start >= total:\n",
    "            break\n",
    "\n",
    "    if pbar:\n",
    "        pbar.close()\n",
    "\n",
    "# =========================\n",
    "# RESUME\n",
    "# =========================\n",
    "if OUT_PATH.exists():\n",
    "    remote_index = json.loads(OUT_PATH.read_text(encoding=\"utf-8\"))\n",
    "    print(f\"[{MODE}] Resuming: loaded {len(remote_index)} records from {OUT_PATH}\")\n",
    "else:\n",
    "    remote_index = {}\n",
    "    print(f\"[{MODE}] Starting fresh index -> {OUT_PATH}\")\n",
    "\n",
    "processed = 0\n",
    "skipped = 0\n",
    "\n",
    "for r in iter_search_results_with_tqdm_local(client, doc_type=CONFIG[\"doc_type\"], max_items=MAX_ITEMS):\n",
    "    link = r.get(\"link\")\n",
    "    if not link:\n",
    "        continue\n",
    "\n",
    "    decision_url = urljoin(GOVUK, link)\n",
    "    slug = slug_from_url(decision_url, CONFIG[\"base_path\"])\n",
    "\n",
    "    if slug in remote_index:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    content_url = urljoin(GOVUK, \"/api/content\" + link)\n",
    "    content = client.get_json(content_url)\n",
    "\n",
    "    title = (content.get(\"title\") or r.get(\"title\") or \"\").strip()\n",
    "    published_date = content.get(\"public_updated_at\") or r.get(\"public_timestamp\")\n",
    "\n",
    "    pdf_url = _pick_pdf_from_content_api(content)\n",
    "    pdf_filename = Path(urlparse(pdf_url).path).name if pdf_url else None\n",
    "\n",
    "    rec = {\n",
    "        \"slug\": slug,\n",
    "        \"decision_page_url\": decision_url,\n",
    "        \"title\": title,\n",
    "        \"decision_date\": None,\n",
    "        \"published_date\": published_date,\n",
    "        \"pdf_url\": pdf_url,\n",
    "        \"pdf_filename\": pdf_filename,\n",
    "        \"pdf_etag\": None,\n",
    "        \"pdf_last_modified\": None,\n",
    "        \"pdf_content_length\": None,\n",
    "        \"head_error\": None,\n",
    "    }\n",
    "\n",
    "    if pdf_url:\n",
    "        h = client.head(pdf_url)\n",
    "        rec[\"pdf_etag\"] = h.headers.get(\"ETag\")\n",
    "        rec[\"pdf_last_modified\"] = h.headers.get(\"Last-Modified\")\n",
    "        rec[\"pdf_content_length\"] = h.headers.get(\"Content-Length\")\n",
    "        rec[\"head_error\"] = h.headers.get(\"x-head-error\")\n",
    "\n",
    "    remote_index[slug] = rec\n",
    "    processed += 1\n",
    "\n",
    "    if processed % CHECKPOINT_EVERY == 0:\n",
    "        atomic_checkpoint(remote_index, OUT_PATH, TMP_PATH)\n",
    "        print(f\"[{MODE}] checkpoint: processed={processed} total={len(remote_index)} skipped={skipped}\")\n",
    "\n",
    "atomic_checkpoint(remote_index, OUT_PATH, TMP_PATH)\n",
    "print(f\"[{MODE}] done: wrote {len(remote_index)} records to {OUT_PATH} (processed={processed}, skipped={skipped})\")\n",
    "print(f\"[{MODE}] manifests folder: {OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4769b97",
   "metadata": {},
   "source": [
    "### Why this step is fast (and why that’s correct)\n",
    "\n",
    "This phase operates entirely on **local state** and **metadata** only.\n",
    "\n",
    "What happens here:\n",
    "- local PDFs are discovered via filesystem walk\n",
    "- only file metadata is read (size, mtime)\n",
    "- no PDF contents are read\n",
    "- no hashing is performed (`compute_sha256=False`)\n",
    "- results are serialized once via `json.dump`\n",
    "\n",
    "What does *not* happen here:\n",
    "- no network calls\n",
    "- no PDF downloads\n",
    "- no content parsing\n",
    "- no OCR\n",
    "- no LLM usage\n",
    "\n",
    "As a result, this step should complete in seconds even for thousands of PDFs.\n",
    "Slowness here would indicate a bug or an unnecessary heavy operation.\n",
    "\n",
    "This is intentional: indexing and reconciliation should always be cheap;\n",
    "expensive work is deferred to later pipeline stages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b99079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import importlib\n",
    "\n",
    "import delta_calc as dc\n",
    "importlib.reload(dc)\n",
    "\n",
    "# =========================\n",
    "# SINGLE SELECTOR (HAMSTER SAFE)\n",
    "# =========================\n",
    "MODE = \"EAT\"   # <-- set to \"EAT\" or \"ET\" ONLY\n",
    "\n",
    "if MODE not in {\"EAT\", \"ET\"}:\n",
    "    raise ValueError(\"MODE must be 'EAT' or 'ET'\")\n",
    "\n",
    "# =========================\n",
    "# HARD PATHS (EXPLICIT)\n",
    "# =========================\n",
    "BASE_MANIFESTS = Path(\"/home/hello/Appeal_reader/manifests\").resolve()\n",
    "OUT_DIR = (BASE_MANIFESTS / MODE).resolve()\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOCAL_DIRS = {\n",
    "    \"EAT\": Path(\"/media/hello/Tribunals/EAT_Appeals\").resolve(),\n",
    "    \"ET\":  Path(\"/media/hello/Tribunals/ET_Cases\").resolve(),\n",
    "}\n",
    "LOCAL_PDF_DIR = LOCAL_DIRS[MODE]\n",
    "\n",
    "if not LOCAL_PDF_DIR.exists():\n",
    "    raise FileNotFoundError(f\"[{MODE}] LOCAL_PDF_DIR does not exist: {LOCAL_PDF_DIR}\")\n",
    "if not LOCAL_PDF_DIR.is_dir():\n",
    "    raise NotADirectoryError(f\"[{MODE}] LOCAL_PDF_DIR is not a directory: {LOCAL_PDF_DIR}\")\n",
    "\n",
    "print(f\"[{MODE}] OUT_DIR       = {OUT_DIR}\")\n",
    "print(f\"[{MODE}] LOCAL_PDF_DIR = {LOCAL_PDF_DIR}\")\n",
    "\n",
    "# =========================\n",
    "# LOAD REMOTE INDEX (SELF-CONTAINED)\n",
    "# =========================\n",
    "REMOTE_INDEX_FILE = {\n",
    "    \"EAT\": OUT_DIR / \"remote_index_eat.json\",\n",
    "    \"ET\":  OUT_DIR / \"remote_index_et.json\",\n",
    "}[MODE]\n",
    "\n",
    "if not REMOTE_INDEX_FILE.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"[{MODE}] Remote index not found: {REMOTE_INDEX_FILE}\\n\"\n",
    "        f\"Run the remote index builder cell first to create it.\"\n",
    "    )\n",
    "\n",
    "remote_index = json.loads(REMOTE_INDEX_FILE.read_text(encoding=\"utf-8\"))\n",
    "if not isinstance(remote_index, dict):\n",
    "    raise TypeError(f\"[{MODE}] remote_index must be a dict, got: {type(remote_index)}\")\n",
    "\n",
    "print(f\"[{MODE}] remote_index loaded: {len(remote_index)} records from {REMOTE_INDEX_FILE}\")\n",
    "\n",
    "# =========================\n",
    "# LOCAL INDEX\n",
    "# =========================\n",
    "local_index = dc.scan_local_pdfs(\n",
    "    LOCAL_PDF_DIR,\n",
    "    recursive=True,\n",
    "    compute_sha256=False,   # True only if you want heavy certainty\n",
    ")\n",
    "\n",
    "dc.write_json(OUT_DIR / \"local_index.json\", local_index)\n",
    "print(f\"[{MODE}] local_index written:\", OUT_DIR / \"local_index.json\")\n",
    "print(f\"[{MODE}] local pdf count:\", len(local_index))\n",
    "\n",
    "# =========================\n",
    "# DELTA\n",
    "# =========================\n",
    "delta = dc.compute_delta(remote_index, local_index)\n",
    "\n",
    "dc.write_json(OUT_DIR / \"delta.json\", delta)\n",
    "print(f\"[{MODE}] delta written:\", OUT_DIR / \"delta.json\")\n",
    "\n",
    "delta[\"counts\"]\n",
    "\n",
    "missing = (delta.get(\"missing\") or [])[:20]\n",
    "changed = (delta.get(\"changed\") or [])[:20]\n",
    "orphaned = (delta.get(\"orphaned\") or [])[:20]\n",
    "\n",
    "missing, changed, orphaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63702d7",
   "metadata": {},
   "source": [
    "### Delta Definitions for above results (Remote vs Local)\n",
    "\n",
    "This pipeline compares the **remote index** (source of truth) against the **local cache**.\n",
    "\n",
    "**missing**  \n",
    "Present in the remote index but absent locally.  \n",
    "→ New or previously undownloaded decisions.  \n",
    "→ Action: download.\n",
    "\n",
    "**changed**  \n",
    "Present both remotely and locally, but metadata differs (e.g. file size).  \n",
    "→ Upstream PDF was silently replaced or corrected.  \n",
    "→ Action: re-download and reprocess.\n",
    "\n",
    "**orphaned**  \n",
    "Present locally but no longer present in the remote index.  \n",
    "→ The EAT site appears to operate a **sliding publication window**, where older\n",
    "decisions may be removed from public listings.  \n",
    "→ Orphaned does *not* mean invalid.  \n",
    "→ Action: retain and quarantine; do not delete automatically.\n",
    "\n",
    "This design preserves completeness while remaining resilient to upstream changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48961d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "import eat_downloader as ed\n",
    "importlib.reload(ed)\n",
    "\n",
    "# =========================\n",
    "# SINGLE SELECTOR (HAMSTER SAFE)\n",
    "# =========================\n",
    "MODE = \"EAT\"   # <-- set to \"EAT\" or \"ET\" ONLY\n",
    "\n",
    "if MODE not in {\"EAT\", \"ET\"}:\n",
    "    raise ValueError(\"MODE must be 'EAT' or 'ET'\")\n",
    "\n",
    "# =========================\n",
    "# HARD PATHS (EXPLICIT)\n",
    "# =========================\n",
    "BASE_MANIFESTS = Path(\"/home/hello/Appeal_reader/manifests\").resolve()\n",
    "OUT_DIR = (BASE_MANIFESTS / MODE).resolve()\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PDF_DIRS = {\n",
    "    \"EAT\": Path(\"/media/hello/Tribunals/EAT_Appeals\").resolve(),\n",
    "    \"ET\":  Path(\"/media/hello/Tribunals/ET_Cases\").resolve(),\n",
    "}\n",
    "CASES_DIR = PDF_DIRS[MODE]\n",
    "CASES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[{MODE}] OUT_DIR    = {OUT_DIR}\")\n",
    "print(f\"[{MODE}] CASES_DIR = {CASES_DIR}\")\n",
    "\n",
    "# =========================\n",
    "# GUARD: ensure we're using the right delta for MODE\n",
    "# =========================\n",
    "delta_path = OUT_DIR / \"delta.json\"\n",
    "if delta_path.exists():\n",
    "    print(f\"[{MODE}] OK: delta.json found -> {delta_path}\")\n",
    "else:\n",
    "    print(f\"[{MODE}] WARNING: delta.json not found in OUT_DIR -> {delta_path}\")\n",
    "    print(f\"[{MODE}] Make sure you ran the delta cell for MODE={MODE} before downloading.\")\n",
    "\n",
    "# =========================\n",
    "# DOWNLOAD (missing + changed)\n",
    "# =========================\n",
    "results = ed.download_missing_and_changed(\n",
    "    delta=delta,\n",
    "    eat_dir=CASES_DIR,        # target folder for PDFs\n",
    "    out_dir=OUT_DIR,          # manifests folder (checkpoint saved here)\n",
    "    archive_changed=True,     # changed => archive old then write new\n",
    "    max_items=10000           # set e.g. 50 for a test\n",
    ")\n",
    "\n",
    "results.keys(), len(results[\"downloaded\"]), len(results[\"archived\"]), len(results[\"failed\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d0f35d",
   "metadata": {},
   "source": [
    "## Phase 1 – Fast PDF Sniffing & Regex Indexing (CPU-Parallel)\n",
    "\n",
    "This script performs a **high-throughput first-pass classification** of ET/EAT PDF decisions.  \n",
    "Its goal is to **quickly label cases by legal topic** (e.g. Unfair Dismissal, Whistleblowing) **without fully parsing or OCR-ing documents**.\n",
    "\n",
    "---\n",
    "\n",
    "### What the script does\n",
    "\n",
    "1. **Selects dataset**\n",
    "   - Controlled by a single `MODE` flag (`\"EAT\"` or `\"ET\"`).\n",
    "   - Points to the corresponding local PDF directory.\n",
    "\n",
    "2. **Scans all PDFs recursively**\n",
    "   - Deterministic ordering (`.rglob(\"*.pdf\")`) for reproducibility.\n",
    "\n",
    "3. **Extracts text from first N pages only**\n",
    "   - Default: first **2 pages** per PDF.\n",
    "   - Uses **PyMuPDF (fitz)** for fast, reliable text extraction.\n",
    "   - Avoids full-document parsing for speed and robustness.\n",
    "\n",
    "4. **Applies compiled regex rules**\n",
    "   - Labels cases such as:\n",
    "     - Unfair dismissal (ERA 1996 s98)\n",
    "     - Constructive dismissal\n",
    "     - Whistleblowing / PIDA\n",
    "     - Discrimination (EqA 2010)\n",
    "     - Redundancy\n",
    "     - TUPE\n",
    "   - Regex is **CPU-cheap**; PDF decoding is the dominant cost.\n",
    "\n",
    "5. **Runs in true parallel**\n",
    "   - Uses `ProcessPoolExecutor` (multiprocessing).\n",
    "   - Fully bypasses the Python GIL.\n",
    "   - Scales across all CPU cores (e.g. 24 cores on Threadripper).\n",
    "\n",
    "6. **Caches results**\n",
    "   - Cache key = `(file path, size, mtime)`.\n",
    "   - Unchanged PDFs are **skipped instantly** on reruns.\n",
    "   - Makes incremental re-indexing cheap.\n",
    "\n",
    "7. **Writes clean outputs**\n",
    "   - `labels__{MODE}.jsonl`  \n",
    "     One row per PDF with labels and basic metadata.\n",
    "   - `failures__{MODE}.jsonl`  \n",
    "     PDFs with no extractable text or parse errors (for OCR/LLM later).\n",
    "   - `stats__{MODE}.json`  \n",
    "     Aggregate counts, timings, and hit distribution.\n",
    "   - `cache__{MODE}.json`  \n",
    "     Incremental processing state.\n",
    "\n",
    "---\n",
    "\n",
    "### What it deliberately does *not* do\n",
    "\n",
    "- ❌ No full-PDF parsing  \n",
    "- ❌ No OCR  \n",
    "- ❌ No embeddings / FAISS  \n",
    "- ❌ No GPU usage  \n",
    "\n",
    "Those are **Phase 2+**, run only on the shortlisted subset.\n",
    "\n",
    "---\n",
    "\n",
    "### Why this design\n",
    "\n",
    "- **Speed first**: 130k+ PDFs become tractable in hours, not days.\n",
    "- **Low risk**: first-page text is high-signal and cheap.\n",
    "- **Fail-loud**: bad PDFs are isolated, not silently skipped.\n",
    "- **Composable**: output feeds directly into semantic indexing or LLM pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "### When to move to Phase 2\n",
    "\n",
    "After this pass, use the labelled subset to:\n",
    "- build FAISS indexes,\n",
    "- run semantic deduplication,\n",
    "- apply LLM-based legal reasoning.\n",
    "\n",
    "This script is the **brute-force front gate**, not the courtroom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b0fe260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ET] PDF_ROOT=/media/hello/Tribunals/ET_Cases\n",
      "[ET] PDFs found: 127,755\n",
      "[ET] MAX_WORKERS=16 FIRST_N_PAGES=2\n",
      "[ET] OUT_DIR=/home/hello/Appeal_reader/indexes/ET\n",
      "[ET] Cache reused: 127,755\n",
      "[ET] To process:   0\n",
      "[ET] DONE.\n",
      "{\n",
      "  \"mode\": \"ET\",\n",
      "  \"pdf_root\": \"/media/hello/Tribunals/ET_Cases\",\n",
      "  \"total_pdfs\": 127755,\n",
      "  \"text_ok\": 127754,\n",
      "  \"text_fail\": 1,\n",
      "  \"hits\": {\n",
      "    \"UNFAIR_DISMISSAL\": 27729,\n",
      "    \"CONSTRUCTIVE_DISMISSAL\": 1441,\n",
      "    \"WRONGFUL_DISMISSAL\": 3339,\n",
      "    \"WHISTLEBLOWING_PIDA\": 2977,\n",
      "    \"DISCRIMINATION\": 20812,\n",
      "    \"REDUNDANCY\": 10382,\n",
      "    \"TUPE\": 1083\n",
      "  },\n",
      "  \"first_n_pages\": 2,\n",
      "  \"max_workers\": 16,\n",
      "  \"labels_path\": \"/home/hello/Appeal_reader/indexes/ET/labels__ET.jsonl\",\n",
      "  \"fails_path\": \"/home/hello/Appeal_reader/indexes/ET/failures__ET.jsonl\",\n",
      "  \"cache_path\": \"/home/hello/Appeal_reader/indexes/ET/cache__ET.json\",\n",
      "  \"elapsed_s\": 1.09\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# =========================\n",
    "# SINGLE SELECTOR (HAMSTER SAFE)\n",
    "# =========================\n",
    "MODE = \"ET\"  # \"EAT\" or \"ET\"\n",
    "if MODE not in {\"EAT\", \"ET\"}:\n",
    "    raise ValueError(\"MODE must be 'EAT' or 'ET'\")\n",
    "\n",
    "# =========================\n",
    "# HARD PATHS (EXPLICIT)\n",
    "# =========================\n",
    "PDF_DIRS = {\n",
    "    \"EAT\": Path(\"/media/hello/Tribunals/EAT_Appeals\").resolve(),\n",
    "    \"ET\":  Path(\"/media/hello/Tribunals/ET_Cases\").resolve(),\n",
    "}\n",
    "PDF_ROOT = PDF_DIRS[MODE]\n",
    "if not PDF_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"[{MODE}] PDF_ROOT does not exist: {PDF_ROOT}\")\n",
    "if not PDF_ROOT.is_dir():\n",
    "    raise NotADirectoryError(f\"[{MODE}] PDF_ROOT is not a directory: {PDF_ROOT}\")\n",
    "\n",
    "# =========================\n",
    "# OUTPUT DIR (MODE-SCOPED)\n",
    "# =========================\n",
    "BASE_OUT_DIR = Path(\"/home/hello/Appeal_reader/indexes\").resolve()\n",
    "\n",
    "OUT_DIR = (BASE_OUT_DIR / MODE).resolve()\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "CACHE_PATH = OUT_DIR / f\"cache__{MODE}.json\"\n",
    "LABELS_PATH = OUT_DIR / f\"labels__{MODE}.jsonl\"\n",
    "FAILS_PATH = OUT_DIR / f\"failures__{MODE}.jsonl\"\n",
    "STATS_PATH = OUT_DIR / f\"stats__{MODE}.json\"\n",
    "\n",
    "# =========================\n",
    "# TUNING\n",
    "# =========================\n",
    "FIRST_N_PAGES = 2            # sniff only first pages\n",
    "MAX_WORKERS = min(16, max(1, (os.cpu_count() or 1) // 2)) \n",
    "MAX_WORKERS = int(os.environ.get(\"PDF_WORKERS\", MAX_WORKERS))\n",
    "CHUNK_PRINT_EVERY = 2000     # progress prints\n",
    "\n",
    "# =========================\n",
    "# REGEX RULES (FAST TRIAGE)\n",
    "# =========================\n",
    "RULES: Dict[str, re.Pattern] = {\n",
    "    \"UNFAIR_DISMISSAL\": re.compile(r\"\\bunfair dismissal\\b|\\bera\\s*1996\\b|\\bsection\\s*98\\b|\\bs\\.?\\s*98\\b|\\bs\\.?\\s*98\\s*\\(\\s*4\\s*\\)\\b\", re.I),\n",
    "    \"CONSTRUCTIVE_DISMISSAL\": re.compile(r\"\\bconstructive dismissal\\b\", re.I),\n",
    "    \"WRONGFUL_DISMISSAL\": re.compile(r\"\\bwrongful dismissal\\b\", re.I),\n",
    "    \"WHISTLEBLOWING_PIDA\": re.compile(r\"\\bwhistleblow|\\bprotected disclosure\\b|\\bpida\\b|\\bpublic interest disclosure\\b\", re.I),\n",
    "    \"DISCRIMINATION\": re.compile(r\"\\bequality act\\b|\\beqa\\s*2010\\b|\\bdiscrimination\\b|\\bharass\\w*\\b|\\bvictimisa\\w*\\b\", re.I),\n",
    "    \"REDUNDANCY\": re.compile(r\"\\bredundan\\w*\\b\", re.I),\n",
    "    \"TUPE\": re.compile(r\"\\btupe\\b|\\btransfer of undertakings\\b\", re.I),\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "\n",
    "def word_count(txt: str) -> int:\n",
    "    # fast + robust, avoids regex overthinking\n",
    "    return len(txt.split())\n",
    "\n",
    "\n",
    "def load_cache(path: Path) -> Dict[str, Any]:\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        obj = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "        return obj if isinstance(obj, dict) else {}\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def save_cache(path: Path, cache: Dict[str, Any]) -> None:\n",
    "    tmp = path.with_suffix(\".tmp\")\n",
    "    tmp.write_text(json.dumps(cache, ensure_ascii=False), encoding=\"utf-8\")\n",
    "    tmp.replace(path)\n",
    "\n",
    "def iter_pdfs(root: Path) -> List[Path]:\n",
    "    # deterministic ordering\n",
    "    return sorted(root.rglob(\"*.pdf\"))\n",
    "\n",
    "def file_sig(p: Path) -> Tuple[int, int]:\n",
    "    st = p.stat()\n",
    "    return (int(st.st_size), int(st.st_mtime))\n",
    "\n",
    "def classify_text(txt: str) -> Dict[str, bool]:\n",
    "    if not txt:\n",
    "        return {k: False for k in RULES}\n",
    "    return {k: bool(rx.search(txt)) for k, rx in RULES.items()}\n",
    "\n",
    "def extract_first_pages_text(pdf_path: str, first_n_pages: int) -> str:\n",
    "    \"\"\"\n",
    "    Uses PyMuPDF (fitz) because it's fast and reliable for text sniffing.\n",
    "    \"\"\"\n",
    "    import fitz  # type: ignore\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "    n = min(first_n_pages, doc.page_count)\n",
    "    parts = []\n",
    "    for i in range(n):\n",
    "        page = doc.load_page(i)\n",
    "        parts.append(page.get_text(\"text\") or \"\")\n",
    "    doc.close()\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "def worker(pdf_path: str, first_n_pages: int) -> Dict[str, Any]:\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        txt = extract_first_pages_text(pdf_path, first_n_pages)\n",
    "        wc = len(txt.split())\n",
    "        labels = classify_text(txt)\n",
    "        out = {\n",
    "            \"path\": pdf_path,\n",
    "            \"labels\": labels,\n",
    "            \"text_ok\": True,\n",
    "            \"text_chars\": len(txt),\n",
    "            \"word_count\": wc,\n",
    "            \"elapsed_s\": round(time.time() - t0, 4),\n",
    "        }\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"path\": pdf_path,\n",
    "            \"labels\": {k: False for k in RULES},\n",
    "            \"text_ok\": False,\n",
    "            \"word_count\": 0,\n",
    "            \"error\": repr(e),\n",
    "            \"elapsed_s\": round(time.time() - t0, 4),\n",
    "        }\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "def main() -> None:\n",
    "    cache = load_cache(CACHE_PATH)\n",
    "\n",
    "    pdfs = iter_pdfs(PDF_ROOT)\n",
    "    total = len(pdfs)\n",
    "    print(f\"[{MODE}] PDF_ROOT={PDF_ROOT}\")\n",
    "    print(f\"[{MODE}] PDFs found: {total:,}\")\n",
    "    print(f\"[{MODE}] MAX_WORKERS={MAX_WORKERS} FIRST_N_PAGES={FIRST_N_PAGES}\")\n",
    "    print(f\"[{MODE}] OUT_DIR={OUT_DIR}\")\n",
    "\n",
    "    # Build task list (incremental)\n",
    "    tasks: List[Path] = []\n",
    "    reused = 0\n",
    "    for p in pdfs:\n",
    "        sig = file_sig(p)\n",
    "        key = str(p)\n",
    "        prev = cache.get(key)\n",
    "        if prev and prev.get(\"sig\") == list(sig):\n",
    "            reused += 1\n",
    "        else:\n",
    "            tasks.append(p)\n",
    "\n",
    "    print(f\"[{MODE}] Cache reused: {reused:,}\")\n",
    "    print(f\"[{MODE}] To process:   {len(tasks):,}\")\n",
    "\n",
    "    # Open outputs (append-friendly, but we will rewrite fresh for simplicity)\n",
    "    # We'll accumulate results then write JSONL once to avoid duplicates.\n",
    "    results: Dict[str, Any] = {}\n",
    "    failures: List[Dict[str, Any]] = []\n",
    "\n",
    "    # Pre-load cached results into results dict\n",
    "    for p in pdfs:\n",
    "        key = str(p)\n",
    "        prev = cache.get(key)\n",
    "        if prev and \"result\" in prev:\n",
    "            results[key] = prev[\"result\"]\n",
    "\n",
    "    processed = 0\n",
    "    t_start = time.time()\n",
    "\n",
    "    if tasks:\n",
    "        with ProcessPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "            futs = {ex.submit(worker, str(p), FIRST_N_PAGES): str(p) for p in tasks}\n",
    "            for fut in as_completed(futs):\n",
    "                path = futs[fut]\n",
    "                res = fut.result()\n",
    "                results[path] = res\n",
    "                cache[path] = {\"sig\": list(file_sig(Path(path))), \"result\": res}\n",
    "                processed += 1\n",
    "\n",
    "                if (processed % CHUNK_PRINT_EVERY) == 0:\n",
    "                    elapsed = time.time() - t_start\n",
    "                    print(f\"[{MODE}] processed {processed:,}/{len(tasks):,} new in {elapsed:.1f}s\")\n",
    "\n",
    "    # Write JSONL fresh (deduped, deterministic order)\n",
    "    ok_count = 0\n",
    "    hit_counts = {k: 0 for k in RULES}\n",
    "    text_fail = 0\n",
    "\n",
    "    with LABELS_PATH.open(\"w\", encoding=\"utf-8\") as f_out, FAILS_PATH.open(\"w\", encoding=\"utf-8\") as f_fail:\n",
    "        for p in pdfs:\n",
    "            key = str(p)\n",
    "            res = results.get(key)\n",
    "            if not res:\n",
    "                # Shouldn't happen, but guard\n",
    "                continue\n",
    "\n",
    "            if not res.get(\"text_ok\"):\n",
    "                text_fail += 1\n",
    "                f_fail.write(json.dumps(res, ensure_ascii=False) + \"\\n\")\n",
    "            else:\n",
    "                ok_count += 1\n",
    "\n",
    "            labels = res.get(\"labels\") or {}\n",
    "            for k in RULES:\n",
    "                if labels.get(k):\n",
    "                    hit_counts[k] += 1\n",
    "\n",
    "            f_out.write(json.dumps(res, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # Save cache + stats\n",
    "    save_cache(CACHE_PATH, cache)\n",
    "\n",
    "    stats = {\n",
    "        \"mode\": MODE,\n",
    "        \"pdf_root\": str(PDF_ROOT),\n",
    "        \"total_pdfs\": total,\n",
    "        \"text_ok\": ok_count,\n",
    "        \"text_fail\": text_fail,\n",
    "        \"hits\": hit_counts,\n",
    "        \"first_n_pages\": FIRST_N_PAGES,\n",
    "        \"max_workers\": MAX_WORKERS,\n",
    "        \"labels_path\": str(LABELS_PATH),\n",
    "        \"fails_path\": str(FAILS_PATH),\n",
    "        \"cache_path\": str(CACHE_PATH),\n",
    "        \"elapsed_s\": round(time.time() - t_start, 2),\n",
    "    }\n",
    "    STATS_PATH.write_text(json.dumps(stats, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"[{MODE}] DONE.\")\n",
    "    print(json.dumps(stats, ensure_ascii=False, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7893c1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing EAT...\n",
      "Saved wide:   /home/hello/Appeal_reader/indexes/EAT/paths_by_label__WIDE__EAT.csv\n",
      "Saved counts: /home/hello/Appeal_reader/indexes/EAT/paths_by_label__COUNTS__EAT.csv\n",
      "\n",
      "Processing ET...\n",
      "Saved wide:   /home/hello/Appeal_reader/indexes/ET/paths_by_label__WIDE__ET.csv\n",
      "Saved counts: /home/hello/Appeal_reader/indexes/ET/paths_by_label__COUNTS__ET.csv\n",
      "\n",
      "Saved master long:   /home/hello/Appeal_reader/indexes/paths_by_label__LONG__EAT_ET.csv\n",
      "Saved master counts:/home/hello/Appeal_reader/indexes/paths_by_label__COUNTS__EAT_ET.csv\n",
      "Rows (long): 68474\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "LABEL_KEYS = [\n",
    "    \"UNFAIR_DISMISSAL\",\n",
    "    \"CONSTRUCTIVE_DISMISSAL\",\n",
    "    \"WRONGFUL_DISMISSAL\",\n",
    "    \"WHISTLEBLOWING_PIDA\",\n",
    "    \"DISCRIMINATION\",\n",
    "    \"REDUNDANCY\",\n",
    "    \"TUPE\",\n",
    "]\n",
    "\n",
    "CONFIGS = [\n",
    "    {\n",
    "        \"court\": \"EAT\",\n",
    "        \"path_in\": \"/home/hello/Appeal_reader/indexes/EAT/labels__EAT.jsonl\",\n",
    "        \"out_wide\": \"/home/hello/Appeal_reader/indexes/EAT/paths_by_label__WIDE__EAT.csv\",\n",
    "        \"out_counts\": \"/home/hello/Appeal_reader/indexes/EAT/paths_by_label__COUNTS__EAT.csv\",\n",
    "    },\n",
    "    {\n",
    "        \"court\": \"ET\",\n",
    "        \"path_in\": \"/home/hello/Appeal_reader/indexes/ET/labels__ET.jsonl\",\n",
    "        \"out_wide\": \"/home/hello/Appeal_reader/indexes/ET/paths_by_label__WIDE__ET.csv\",\n",
    "        \"out_counts\": \"/home/hello/Appeal_reader/indexes/ET/paths_by_label__COUNTS__ET.csv\",\n",
    "    },\n",
    "]\n",
    "\n",
    "OUT_LONG_MASTER = \"/home/hello/Appeal_reader/indexes/paths_by_label__LONG__EAT_ET.csv\"\n",
    "OUT_COUNTS_MASTER = \"/home/hello/Appeal_reader/indexes/paths_by_label__COUNTS__EAT_ET.csv\"\n",
    "\n",
    "\n",
    "def _flatten_labels(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    label_fields = df.select(\"labels\").schema[\"labels\"].fields\n",
    "    label_names = [f.name for f in label_fields]\n",
    "    return df.with_columns(\n",
    "        [pl.col(\"labels\").struct.field(n).alias(n) for n in label_names]\n",
    "    ).drop(\"labels\")\n",
    "\n",
    "\n",
    "def _detect_path_col(df: pl.DataFrame, path_in: str) -> str:\n",
    "    path_candidates = [\"path\", \"pdf_path\", \"file_path\", \"filepath\", \"filename\", \"source_path\"]\n",
    "    path_col = next((c for c in path_candidates if c in df.columns), None)\n",
    "    if path_col is None:\n",
    "        raise ValueError(f\"No path-like column found in {path_in}. Columns: {df.columns}\")\n",
    "    return path_col\n",
    "\n",
    "\n",
    "def _normalize_label_bools(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    missing = [k for k in LABEL_KEYS if k not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing label columns: {missing}\")\n",
    "\n",
    "    return df.with_columns([\n",
    "        pl.col(k)\n",
    "        .cast(pl.Utf8, strict=False)\n",
    "        .str.strip_chars()\n",
    "        .str.to_lowercase()\n",
    "        .is_in([\"true\", \"1\", \"yes\", \"y\"])\n",
    "        .alias(k)\n",
    "        for k in LABEL_KEYS\n",
    "    ])\n",
    "\n",
    "\n",
    "def build_wide_csv(df: pl.DataFrame, path_col: str, out_csv: str) -> dict:\n",
    "    lists = {\n",
    "        k: (\n",
    "            df.filter(pl.col(k) == True)\n",
    "            .select(pl.col(path_col).cast(pl.Utf8))\n",
    "            .to_series()\n",
    "            .to_list()\n",
    "        )\n",
    "        for k in LABEL_KEYS\n",
    "    }\n",
    "\n",
    "    max_len = max((len(v) for v in lists.values()), default=0)\n",
    "\n",
    "    wide_df = pl.DataFrame({\n",
    "        k: v + [None] * (max_len - len(v))\n",
    "        for k, v in lists.items()\n",
    "    })\n",
    "\n",
    "    wide_df.write_csv(out_csv)\n",
    "    return {k: len(v) for k, v in lists.items()}\n",
    "\n",
    "\n",
    "def write_counts_csv(counts: dict, court: str, out_csv: str) -> pl.DataFrame:\n",
    "    df = pl.DataFrame({\n",
    "        \"court\": [court] * len(counts),\n",
    "        \"label\": list(counts.keys()),\n",
    "        \"count\": list(counts.values()),\n",
    "    })\n",
    "    df.write_csv(out_csv)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_long_df(df: pl.DataFrame, path_col: str, court: str) -> pl.DataFrame:\n",
    "    parts = []\n",
    "    for k in LABEL_KEYS:\n",
    "        parts.append(\n",
    "            df.filter(pl.col(k) == True).select([\n",
    "                pl.lit(court).alias(\"court\"),\n",
    "                pl.lit(k).alias(\"label\"),\n",
    "                pl.col(path_col).cast(pl.Utf8).alias(\"path\"),\n",
    "            ])\n",
    "        )\n",
    "    return pl.concat(parts, how=\"vertical\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    long_parts = []\n",
    "    count_parts = []\n",
    "\n",
    "    for cfg in CONFIGS:\n",
    "        court = cfg[\"court\"]\n",
    "        print(f\"\\nProcessing {court}...\")\n",
    "\n",
    "        df = pl.read_ndjson(cfg[\"path_in\"])\n",
    "        df = _flatten_labels(df)\n",
    "        path_col = _detect_path_col(df, cfg[\"path_in\"])\n",
    "        df = _normalize_label_bools(df)\n",
    "\n",
    "        counts = build_wide_csv(df, path_col, cfg[\"out_wide\"])\n",
    "        counts_df = write_counts_csv(counts, court, cfg[\"out_counts\"])\n",
    "\n",
    "        print(f\"Saved wide:   {cfg['out_wide']}\")\n",
    "        print(f\"Saved counts: {cfg['out_counts']}\")\n",
    "\n",
    "        long_parts.append(build_long_df(df, path_col, court))\n",
    "        count_parts.append(counts_df)\n",
    "\n",
    "    # master long (EAT + ET)\n",
    "    master_long = (\n",
    "        pl.concat(long_parts, how=\"vertical\")\n",
    "        if long_parts\n",
    "        else pl.DataFrame({\"court\": [], \"label\": [], \"path\": []})\n",
    "    )\n",
    "    master_long.write_csv(OUT_LONG_MASTER)\n",
    "\n",
    "    # master counts (EAT + ET)\n",
    "    master_counts = pl.concat(count_parts, how=\"vertical\")\n",
    "    master_counts.write_csv(OUT_COUNTS_MASTER)\n",
    "\n",
    "    print(f\"\\nSaved master long:   {OUT_LONG_MASTER}\")\n",
    "    print(f\"Saved master counts:{OUT_COUNTS_MASTER}\")\n",
    "    print(f\"Rows (long): {master_long.height}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
