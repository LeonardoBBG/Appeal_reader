{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e8ca1c2",
   "metadata": {},
   "source": [
    "### Remote Index Strategy\n",
    "\n",
    "This notebook always scans the full GOV.UK Employment Appeal Tribunal universe.\n",
    "\n",
    "If `remote_index.json` already exists:\n",
    "- it is treated as a cache, not a source of truth\n",
    "- previously indexed slugs are skipped\n",
    "- only new or missing decisions are fetched and HEAD-checked\n",
    "\n",
    "This design ensures:\n",
    "- crash-safe resume (no lost progress)\n",
    "- detection of newly added decisions\n",
    "- immunity to GOV.UK reordering or backfills\n",
    "\n",
    "The Search API scan is cheap; Content API fetches and PDF HEAD requests are the expensive steps and are only performed for unseen decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4a00938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EAT] Starting fresh index -> /home/hello/Appeal_reader/manifests/EAT/remote_index_eat.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03d1152710c462eb0bceae87364c5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EAT remote index:   0%|          | 0/2480 [00:00<?, ?doc/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EAT] checkpoint: processed=200 total=200 skipped=0\n",
      "[EAT] checkpoint: processed=400 total=400 skipped=0\n",
      "[EAT] checkpoint: processed=600 total=600 skipped=0\n",
      "[EAT] checkpoint: processed=800 total=800 skipped=0\n",
      "[EAT] checkpoint: processed=1000 total=1000 skipped=0\n",
      "[EAT] checkpoint: processed=1200 total=1200 skipped=0\n",
      "[EAT] checkpoint: processed=1400 total=1400 skipped=0\n",
      "[EAT] checkpoint: processed=1600 total=1600 skipped=0\n",
      "[EAT] checkpoint: processed=1800 total=1800 skipped=0\n",
      "[EAT] checkpoint: processed=2000 total=2000 skipped=0\n",
      "[EAT] checkpoint: processed=2200 total=2200 skipped=0\n",
      "[EAT] checkpoint: processed=2400 total=2400 skipped=0\n",
      "[EAT] done: wrote 2480 records to /home/hello/Appeal_reader/manifests/EAT/remote_index_eat.json (processed=2480, skipped=0)\n",
      "[EAT] manifests folder: /home/hello/Appeal_reader/manifests/EAT\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from urllib.parse import urljoin, urlparse, quote\n",
    "import json\n",
    "\n",
    "from eat_remote_index import (\n",
    "    HttpClient,\n",
    "    GOVUK,\n",
    "    write_json,\n",
    "    _pick_pdf_from_content_api,   # returns first PDF (fine for now)\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# SINGLE SELECTOR (HAMSTER SAFE)\n",
    "# =========================\n",
    "MODE = \"EAT\"   # <-- set to \"EAT\" or \"ET\" ONLY\n",
    "\n",
    "if MODE not in {\"EAT\", \"ET\"}:\n",
    "    raise ValueError(\"MODE must be 'EAT' or 'ET'\")\n",
    "\n",
    "CONFIG = {\n",
    "    \"EAT\": {\n",
    "        \"doc_type\": \"employment_appeal_tribunal_decision\",\n",
    "        \"base_path\": \"/employment-appeal-tribunal-decisions/\",\n",
    "        \"out_name\": \"remote_index_eat.json\",\n",
    "        \"tmp_name\": \"remote_index_eat.tmp.json\",\n",
    "    },\n",
    "    \"ET\": {\n",
    "        \"doc_type\": \"employment_tribunal_decision\",\n",
    "        \"base_path\": \"/employment-tribunal-decisions/\",\n",
    "        \"out_name\": \"remote_index_et.json\",\n",
    "        \"tmp_name\": \"remote_index_et.tmp.json\",\n",
    "    },\n",
    "}[MODE]\n",
    "\n",
    "# =========================\n",
    "# MANIFEST PATHS (HARD, EXPLICIT)\n",
    "# =========================\n",
    "BASE_MANIFESTS = Path(\"/home/hello/Appeal_reader/manifests\").resolve()\n",
    "OUT_DIR = (BASE_MANIFESTS / MODE).resolve()\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_PATH = OUT_DIR / CONFIG[\"out_name\"]\n",
    "TMP_PATH = OUT_DIR / CONFIG[\"tmp_name\"]\n",
    "\n",
    "CHECKPOINT_EVERY = 200\n",
    "MAX_ITEMS = None  # set to 200 for smoke test\n",
    "\n",
    "client = HttpClient(timeout=30.0, max_retries=4, backoff_base=0.8, min_delay=0.15)\n",
    "\n",
    "def atomic_checkpoint(obj, out_path: Path, tmp_path: Path):\n",
    "    write_json(tmp_path, obj)\n",
    "    tmp_path.replace(out_path)\n",
    "\n",
    "def slug_from_url(url: str, base_path: str) -> str:\n",
    "    path = urlparse(url).path.rstrip(\"/\")\n",
    "    return path.split(base_path)[-1].strip(\"/\")\n",
    "\n",
    "def iter_search_results_with_tqdm_local(client, *, doc_type: str, count: int = 200, max_items=None, order: str = \"-public_timestamp\"):\n",
    "    \"\"\"\n",
    "    GOV.UK Search API iterator with tqdm that initializes after first response\n",
    "    (so we know 'total').\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from tqdm.auto import tqdm\n",
    "    except Exception:\n",
    "        def tqdm(x, **kwargs):\n",
    "            return x\n",
    "\n",
    "    start = 0\n",
    "    yielded = 0\n",
    "    pbar = None\n",
    "\n",
    "    while True:\n",
    "        url = (\n",
    "            f\"{GOVUK}/api/search.json\"\n",
    "            f\"?filter_document_type={quote(doc_type)}\"\n",
    "            f\"&order={quote(order)}\"\n",
    "            f\"&count={count}\"\n",
    "            f\"&start={start}\"\n",
    "        )\n",
    "        payload = client.get_json(url)\n",
    "        results = payload.get(\"results\") or []\n",
    "        if not results:\n",
    "            break\n",
    "\n",
    "        if pbar is None:\n",
    "            total = payload.get(\"total\")\n",
    "            pbar = tqdm(total=total, desc=f\"{MODE} remote index\", unit=\"doc\")\n",
    "\n",
    "        for r in results:\n",
    "            yield r\n",
    "            yielded += 1\n",
    "            pbar.update(1)\n",
    "            if max_items is not None and yielded >= max_items:\n",
    "                pbar.close()\n",
    "                return\n",
    "\n",
    "        start += len(results)\n",
    "        total = payload.get(\"total\")\n",
    "        if isinstance(total, int) and start >= total:\n",
    "            break\n",
    "\n",
    "    if pbar:\n",
    "        pbar.close()\n",
    "\n",
    "# =========================\n",
    "# RESUME\n",
    "# =========================\n",
    "if OUT_PATH.exists():\n",
    "    remote_index = json.loads(OUT_PATH.read_text(encoding=\"utf-8\"))\n",
    "    print(f\"[{MODE}] Resuming: loaded {len(remote_index)} records from {OUT_PATH}\")\n",
    "else:\n",
    "    remote_index = {}\n",
    "    print(f\"[{MODE}] Starting fresh index -> {OUT_PATH}\")\n",
    "\n",
    "processed = 0\n",
    "skipped = 0\n",
    "\n",
    "for r in iter_search_results_with_tqdm_local(client, doc_type=CONFIG[\"doc_type\"], max_items=MAX_ITEMS):\n",
    "    link = r.get(\"link\")\n",
    "    if not link:\n",
    "        continue\n",
    "\n",
    "    decision_url = urljoin(GOVUK, link)\n",
    "    slug = slug_from_url(decision_url, CONFIG[\"base_path\"])\n",
    "\n",
    "    if slug in remote_index:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    content_url = urljoin(GOVUK, \"/api/content\" + link)\n",
    "    content = client.get_json(content_url)\n",
    "\n",
    "    title = (content.get(\"title\") or r.get(\"title\") or \"\").strip()\n",
    "    published_date = content.get(\"public_updated_at\") or r.get(\"public_timestamp\")\n",
    "\n",
    "    pdf_url = _pick_pdf_from_content_api(content)\n",
    "    pdf_filename = Path(urlparse(pdf_url).path).name if pdf_url else None\n",
    "\n",
    "    rec = {\n",
    "        \"slug\": slug,\n",
    "        \"decision_page_url\": decision_url,\n",
    "        \"title\": title,\n",
    "        \"decision_date\": None,\n",
    "        \"published_date\": published_date,\n",
    "        \"pdf_url\": pdf_url,\n",
    "        \"pdf_filename\": pdf_filename,\n",
    "        \"pdf_etag\": None,\n",
    "        \"pdf_last_modified\": None,\n",
    "        \"pdf_content_length\": None,\n",
    "        \"head_error\": None,\n",
    "    }\n",
    "\n",
    "    if pdf_url:\n",
    "        h = client.head(pdf_url)\n",
    "        rec[\"pdf_etag\"] = h.headers.get(\"ETag\")\n",
    "        rec[\"pdf_last_modified\"] = h.headers.get(\"Last-Modified\")\n",
    "        rec[\"pdf_content_length\"] = h.headers.get(\"Content-Length\")\n",
    "        rec[\"head_error\"] = h.headers.get(\"x-head-error\")\n",
    "\n",
    "    remote_index[slug] = rec\n",
    "    processed += 1\n",
    "\n",
    "    if processed % CHECKPOINT_EVERY == 0:\n",
    "        atomic_checkpoint(remote_index, OUT_PATH, TMP_PATH)\n",
    "        print(f\"[{MODE}] checkpoint: processed={processed} total={len(remote_index)} skipped={skipped}\")\n",
    "\n",
    "atomic_checkpoint(remote_index, OUT_PATH, TMP_PATH)\n",
    "print(f\"[{MODE}] done: wrote {len(remote_index)} records to {OUT_PATH} (processed={processed}, skipped={skipped})\")\n",
    "print(f\"[{MODE}] manifests folder: {OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4769b97",
   "metadata": {},
   "source": [
    "### Why this step is fast (and why that’s correct)\n",
    "\n",
    "This phase operates entirely on **local state** and **metadata** only.\n",
    "\n",
    "What happens here:\n",
    "- local PDFs are discovered via filesystem walk\n",
    "- only file metadata is read (size, mtime)\n",
    "- no PDF contents are read\n",
    "- no hashing is performed (`compute_sha256=False`)\n",
    "- results are serialized once via `json.dump`\n",
    "\n",
    "What does *not* happen here:\n",
    "- no network calls\n",
    "- no PDF downloads\n",
    "- no content parsing\n",
    "- no OCR\n",
    "- no LLM usage\n",
    "\n",
    "As a result, this step should complete in seconds even for thousands of PDFs.\n",
    "Slowness here would indicate a bug or an unnecessary heavy operation.\n",
    "\n",
    "This is intentional: indexing and reconciliation should always be cheap;\n",
    "expensive work is deferred to later pipeline stages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74b99079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EAT] OUT_DIR       = /home/hello/Appeal_reader/manifests/EAT\n",
      "[EAT] LOCAL_PDF_DIR = /media/hello/Tribunals/EAT_Appeals\n",
      "[EAT] remote_index loaded: 2480 records from /home/hello/Appeal_reader/manifests/EAT/remote_index_eat.json\n",
      "[EAT] local_index written: /home/hello/Appeal_reader/manifests/EAT/local_index.json\n",
      "[EAT] local pdf count: 2487\n",
      "[EAT] delta written: /home/hello/Appeal_reader/manifests/EAT/delta.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [],\n",
       " [{'filename': 'Digital_Communication_Systems_Ltd_v_Mr_C_Scully_UKEAT_0182_19_LA.pdf',\n",
       "   'path': '/media/hello/Tribunals/EAT_Appeals/Digital_Communication_Systems_Ltd_v_Mr_C_Scully_UKEAT_0182_19_LA.pdf'},\n",
       "  {'filename': 'Dr_Alaa_Jalaal_v_Grampian_Health_Board_Tayside_Health_Board_NHS_Education_For_Scotland__2024__EAT_97.pdf',\n",
       "   'path': '/media/hello/Tribunals/EAT_Appeals/Dr_Alaa_Jalaal_v_Grampian_Health_Board_Tayside_Health_Board_NHS_Education_For_Scotland__2024__EAT_97.pdf'},\n",
       "  {'filename': 'MITIE_Property_Services_UK_Ltd__v__Mr_B_Bennett_and_20_Others_and_Others_UKEATS_0023_19_SS_.pdf',\n",
       "   'path': '/media/hello/Tribunals/EAT_Appeals/MITIE_Property_Services_UK_Ltd__v__Mr_B_Bennett_and_20_Others_and_Others_UKEATS_0023_19_SS_.pdf'},\n",
       "  {'filename': 'Mr_John_J_Campbell_v_1__Sheffield_Teaching_North_Hospitals_NHS_Foundation_Trust_2__Mr_Wesley_Hammond__2025__EAT_42.pdf',\n",
       "   'path': '/media/hello/Tribunals/EAT_Appeals/Mr_John_J_Campbell_v_1__Sheffield_Teaching_North_Hospitals_NHS_Foundation_Trust_2__Mr_Wesley_Hammond__2025__EAT_42.pdf'},\n",
       "  {'filename': 'Mr_L_Humby_v_Barts_Health_NHS_Trust__2024__EAT_17__mr-l-humby-v-barts-health-nhs-trust-2024-eat-16__archived_20260114T100446Z.pdf',\n",
       "   'path': '/media/hello/Tribunals/EAT_Appeals/archive/Mr_L_Humby_v_Barts_Health_NHS_Trust__2024__EAT_17__mr-l-humby-v-barts-health-nhs-trust-2024-eat-16__archived_20260114T100446Z.pdf'},\n",
       "  {'filename': 'Mr_N_Mendy_v_Motorola_Solutions_UK_Ltd_and_Others___2023__EAT_71.pdf',\n",
       "   'path': '/media/hello/Tribunals/EAT_Appeals/Mr_N_Mendy_v_Motorola_Solutions_UK_Ltd_and_Others___2023__EAT_71.pdf'},\n",
       "  {'filename': 'Ms_A_Robinson_v_Nottingham_Healthcare_NHS_Foundation_Trust__2025__EAT_39.pdf',\n",
       "   'path': '/media/hello/Tribunals/EAT_Appeals/Ms_A_Robinson_v_Nottingham_Healthcare_NHS_Foundation_Trust__2025__EAT_39.pdf'},\n",
       "  {'filename': 'T-Systems_v_Mrs_K_Lewis_UKEAT_0042_15__JOJ.pdf',\n",
       "   'path': '/media/hello/Tribunals/EAT_Appeals/T-Systems_v_Mrs_K_Lewis_UKEAT_0042_15__JOJ.pdf'},\n",
       "  {'filename': 'Wilson_Barca_LLP_and_Others_v_Ms_M_Shirin_UKEAT_0276_19_BA__wilson-barca-llp-and-others-v-ms-m-shirin-ukeat-0276-19-ba__archived_20260114T100447Z.pdf',\n",
       "   'path': '/media/hello/Tribunals/EAT_Appeals/archive/Wilson_Barca_LLP_and_Others_v_Ms_M_Shirin_UKEAT_0276_19_BA__wilson-barca-llp-and-others-v-ms-m-shirin-ukeat-0276-19-ba__archived_20260114T100447Z.pdf'},\n",
       "  {'filename': '_1__Exmoor_Ales_Ltd__2__Mr_J_Price_v_Mrs_J_Hedy_Herriot_UKEAT_0075_18_RN-checkpoint.pdf',\n",
       "   'path': '/media/hello/Tribunals/EAT_Appeals/_1__Exmoor_Ales_Ltd__2__Mr_J_Price_v_Mrs_J_Hedy_Herriot_UKEAT_0075_18_RN-checkpoint.pdf'}])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import importlib\n",
    "\n",
    "import delta_calc as dc\n",
    "importlib.reload(dc)\n",
    "\n",
    "# =========================\n",
    "# SINGLE SELECTOR (HAMSTER SAFE)\n",
    "# =========================\n",
    "MODE = \"EAT\"   # <-- set to \"EAT\" or \"ET\" ONLY\n",
    "\n",
    "if MODE not in {\"EAT\", \"ET\"}:\n",
    "    raise ValueError(\"MODE must be 'EAT' or 'ET'\")\n",
    "\n",
    "# =========================\n",
    "# HARD PATHS (EXPLICIT)\n",
    "# =========================\n",
    "BASE_MANIFESTS = Path(\"/home/hello/Appeal_reader/manifests\").resolve()\n",
    "OUT_DIR = (BASE_MANIFESTS / MODE).resolve()\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOCAL_DIRS = {\n",
    "    \"EAT\": Path(\"/media/hello/Tribunals/EAT_Appeals\").resolve(),\n",
    "    \"ET\":  Path(\"/media/hello/Tribunals/ET_Cases\").resolve(),\n",
    "}\n",
    "LOCAL_PDF_DIR = LOCAL_DIRS[MODE]\n",
    "\n",
    "if not LOCAL_PDF_DIR.exists():\n",
    "    raise FileNotFoundError(f\"[{MODE}] LOCAL_PDF_DIR does not exist: {LOCAL_PDF_DIR}\")\n",
    "if not LOCAL_PDF_DIR.is_dir():\n",
    "    raise NotADirectoryError(f\"[{MODE}] LOCAL_PDF_DIR is not a directory: {LOCAL_PDF_DIR}\")\n",
    "\n",
    "print(f\"[{MODE}] OUT_DIR       = {OUT_DIR}\")\n",
    "print(f\"[{MODE}] LOCAL_PDF_DIR = {LOCAL_PDF_DIR}\")\n",
    "\n",
    "# =========================\n",
    "# LOAD REMOTE INDEX (SELF-CONTAINED)\n",
    "# =========================\n",
    "REMOTE_INDEX_FILE = {\n",
    "    \"EAT\": OUT_DIR / \"remote_index_eat.json\",\n",
    "    \"ET\":  OUT_DIR / \"remote_index_et.json\",\n",
    "}[MODE]\n",
    "\n",
    "if not REMOTE_INDEX_FILE.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"[{MODE}] Remote index not found: {REMOTE_INDEX_FILE}\\n\"\n",
    "        f\"Run the remote index builder cell first to create it.\"\n",
    "    )\n",
    "\n",
    "remote_index = json.loads(REMOTE_INDEX_FILE.read_text(encoding=\"utf-8\"))\n",
    "if not isinstance(remote_index, dict):\n",
    "    raise TypeError(f\"[{MODE}] remote_index must be a dict, got: {type(remote_index)}\")\n",
    "\n",
    "print(f\"[{MODE}] remote_index loaded: {len(remote_index)} records from {REMOTE_INDEX_FILE}\")\n",
    "\n",
    "# =========================\n",
    "# LOCAL INDEX\n",
    "# =========================\n",
    "local_index = dc.scan_local_pdfs(\n",
    "    LOCAL_PDF_DIR,\n",
    "    recursive=True,\n",
    "    compute_sha256=False,   # True only if you want heavy certainty\n",
    ")\n",
    "\n",
    "dc.write_json(OUT_DIR / \"local_index.json\", local_index)\n",
    "print(f\"[{MODE}] local_index written:\", OUT_DIR / \"local_index.json\")\n",
    "print(f\"[{MODE}] local pdf count:\", len(local_index))\n",
    "\n",
    "# =========================\n",
    "# DELTA\n",
    "# =========================\n",
    "delta = dc.compute_delta(remote_index, local_index)\n",
    "\n",
    "dc.write_json(OUT_DIR / \"delta.json\", delta)\n",
    "print(f\"[{MODE}] delta written:\", OUT_DIR / \"delta.json\")\n",
    "\n",
    "delta[\"counts\"]\n",
    "\n",
    "missing = (delta.get(\"missing\") or [])[:20]\n",
    "changed = (delta.get(\"changed\") or [])[:20]\n",
    "orphaned = (delta.get(\"orphaned\") or [])[:20]\n",
    "\n",
    "missing, changed, orphaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63702d7",
   "metadata": {},
   "source": [
    "### Delta Definitions for above results (Remote vs Local)\n",
    "\n",
    "This pipeline compares the **remote index** (source of truth) against the **local cache**.\n",
    "\n",
    "**missing**  \n",
    "Present in the remote index but absent locally.  \n",
    "→ New or previously undownloaded decisions.  \n",
    "→ Action: download.\n",
    "\n",
    "**changed**  \n",
    "Present both remotely and locally, but metadata differs (e.g. file size).  \n",
    "→ Upstream PDF was silently replaced or corrected.  \n",
    "→ Action: re-download and reprocess.\n",
    "\n",
    "**orphaned**  \n",
    "Present locally but no longer present in the remote index.  \n",
    "→ The EAT site appears to operate a **sliding publication window**, where older\n",
    "decisions may be removed from public listings.  \n",
    "→ Orphaned does *not* mean invalid.  \n",
    "→ Action: retain and quarantine; do not delete automatically.\n",
    "\n",
    "This design preserves completeness while remaining resilient to upstream changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48961d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EAT] OUT_DIR    = /home/hello/Appeal_reader/manifests/EAT\n",
      "[EAT] CASES_DIR = /media/hello/Tribunals/EAT_Appeals\n",
      "[EAT] OK: delta.json found -> /home/hello/Appeal_reader/manifests/EAT/delta.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703fa47be94a42cdae10303cb7d92616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading EAT PDFs:   0%|          | 0/4 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(dict_keys(['downloaded', 'archived', 'failed']), 175, 2, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "import eat_downloader as ed\n",
    "importlib.reload(ed)\n",
    "\n",
    "# =========================\n",
    "# SINGLE SELECTOR (HAMSTER SAFE)\n",
    "# =========================\n",
    "MODE = \"EAT\"   # <-- set to \"EAT\" or \"ET\" ONLY\n",
    "\n",
    "if MODE not in {\"EAT\", \"ET\"}:\n",
    "    raise ValueError(\"MODE must be 'EAT' or 'ET'\")\n",
    "\n",
    "# =========================\n",
    "# HARD PATHS (EXPLICIT)\n",
    "# =========================\n",
    "BASE_MANIFESTS = Path(\"/home/hello/Appeal_reader/manifests\").resolve()\n",
    "OUT_DIR = (BASE_MANIFESTS / MODE).resolve()\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PDF_DIRS = {\n",
    "    \"EAT\": Path(\"/media/hello/Tribunals/EAT_Appeals\").resolve(),\n",
    "    \"ET\":  Path(\"/media/hello/Tribunals/ET_Cases\").resolve(),\n",
    "}\n",
    "CASES_DIR = PDF_DIRS[MODE]\n",
    "CASES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[{MODE}] OUT_DIR    = {OUT_DIR}\")\n",
    "print(f\"[{MODE}] CASES_DIR = {CASES_DIR}\")\n",
    "\n",
    "# =========================\n",
    "# GUARD: ensure we're using the right delta for MODE\n",
    "# =========================\n",
    "delta_path = OUT_DIR / \"delta.json\"\n",
    "if delta_path.exists():\n",
    "    print(f\"[{MODE}] OK: delta.json found -> {delta_path}\")\n",
    "else:\n",
    "    print(f\"[{MODE}] WARNING: delta.json not found in OUT_DIR -> {delta_path}\")\n",
    "    print(f\"[{MODE}] Make sure you ran the delta cell for MODE={MODE} before downloading.\")\n",
    "\n",
    "# =========================\n",
    "# DOWNLOAD (missing + changed)\n",
    "# =========================\n",
    "results = ed.download_missing_and_changed(\n",
    "    delta=delta,\n",
    "    eat_dir=CASES_DIR,        # target folder for PDFs\n",
    "    out_dir=OUT_DIR,          # manifests folder (checkpoint saved here)\n",
    "    archive_changed=True,     # changed => archive old then write new\n",
    "    max_items=10000           # set e.g. 50 for a test\n",
    ")\n",
    "\n",
    "results.keys(), len(results[\"downloaded\"]), len(results[\"archived\"]), len(results[\"failed\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d0f35d",
   "metadata": {},
   "source": [
    "## Phase 1 – Fast PDF Sniffing & Regex Indexing (CPU-Parallel)\n",
    "\n",
    "This script performs a **high-throughput first-pass classification** of ET/EAT PDF decisions.  \n",
    "Its goal is to **quickly label cases by legal topic** (e.g. Unfair Dismissal, Whistleblowing) **without fully parsing or OCR-ing documents**.\n",
    "\n",
    "---\n",
    "\n",
    "### What the script does\n",
    "\n",
    "1. **Selects dataset**\n",
    "   - Controlled by a single `MODE` flag (`\"EAT\"` or `\"ET\"`).\n",
    "   - Points to the corresponding local PDF directory.\n",
    "\n",
    "2. **Scans all PDFs recursively**\n",
    "   - Deterministic ordering (`.rglob(\"*.pdf\")`) for reproducibility.\n",
    "\n",
    "3. **Extracts text from first N pages only**\n",
    "   - Default: first **2 pages** per PDF.\n",
    "   - Uses **PyMuPDF (fitz)** for fast, reliable text extraction.\n",
    "   - Avoids full-document parsing for speed and robustness.\n",
    "\n",
    "4. **Applies compiled regex rules**\n",
    "   - Labels cases such as:\n",
    "     - Unfair dismissal (ERA 1996 s98)\n",
    "     - Constructive dismissal\n",
    "     - Whistleblowing / PIDA\n",
    "     - Discrimination (EqA 2010)\n",
    "     - Redundancy\n",
    "     - TUPE\n",
    "   - Regex is **CPU-cheap**; PDF decoding is the dominant cost.\n",
    "\n",
    "5. **Runs in true parallel**\n",
    "   - Uses `ProcessPoolExecutor` (multiprocessing).\n",
    "   - Fully bypasses the Python GIL.\n",
    "   - Scales across all CPU cores (e.g. 24 cores on Threadripper).\n",
    "\n",
    "6. **Caches results**\n",
    "   - Cache key = `(file path, size, mtime)`.\n",
    "   - Unchanged PDFs are **skipped instantly** on reruns.\n",
    "   - Makes incremental re-indexing cheap.\n",
    "\n",
    "7. **Writes clean outputs**\n",
    "   - `labels__{MODE}.jsonl`  \n",
    "     One row per PDF with labels and basic metadata.\n",
    "   - `failures__{MODE}.jsonl`  \n",
    "     PDFs with no extractable text or parse errors (for OCR/LLM later).\n",
    "   - `stats__{MODE}.json`  \n",
    "     Aggregate counts, timings, and hit distribution.\n",
    "   - `cache__{MODE}.json`  \n",
    "     Incremental processing state.\n",
    "\n",
    "---\n",
    "\n",
    "### What it deliberately does *not* do\n",
    "\n",
    "- ❌ No full-PDF parsing  \n",
    "- ❌ No OCR  \n",
    "- ❌ No embeddings / FAISS  \n",
    "- ❌ No GPU usage  \n",
    "\n",
    "Those are **Phase 2+**, run only on the shortlisted subset.\n",
    "\n",
    "---\n",
    "\n",
    "### Why this design\n",
    "\n",
    "- **Speed first**: 130k+ PDFs become tractable in hours, not days.\n",
    "- **Low risk**: first-page text is high-signal and cheap.\n",
    "- **Fail-loud**: bad PDFs are isolated, not silently skipped.\n",
    "- **Composable**: output feeds directly into semantic indexing or LLM pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "### When to move to Phase 2\n",
    "\n",
    "After this pass, use the labelled subset to:\n",
    "- build FAISS indexes,\n",
    "- run semantic deduplication,\n",
    "- apply LLM-based legal reasoning.\n",
    "\n",
    "This script is the **brute-force front gate**, not the courtroom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b0fe260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EAT] PDF_ROOT=/media/hello/Tribunals/EAT_Appeals\n",
      "[EAT] PDFs found: 2,487\n",
      "[EAT] MAX_WORKERS=48 FIRST_N_PAGES=2\n",
      "[EAT] OUT_DIR=/home/hello/Appeal_reader/indexes/EAT\n",
      "[EAT] Cache reused: 2,483\n",
      "[EAT] To process:   4\n",
      "[EAT] DONE.\n",
      "{\n",
      "  \"mode\": \"EAT\",\n",
      "  \"pdf_root\": \"/media/hello/Tribunals/EAT_Appeals\",\n",
      "  \"total_pdfs\": 2487,\n",
      "  \"text_ok\": 2486,\n",
      "  \"text_fail\": 1,\n",
      "  \"hits\": {\n",
      "    \"UNFAIR_DISMISSAL\": 192,\n",
      "    \"CONSTRUCTIVE_DISMISSAL\": 19,\n",
      "    \"WRONGFUL_DISMISSAL\": 19,\n",
      "    \"WHISTLEBLOWING_PIDA\": 60,\n",
      "    \"DISCRIMINATION\": 358,\n",
      "    \"REDUNDANCY\": 49,\n",
      "    \"TUPE\": 14\n",
      "  },\n",
      "  \"first_n_pages\": 2,\n",
      "  \"max_workers\": 48,\n",
      "  \"labels_path\": \"/home/hello/Appeal_reader/indexes/EAT/labels__EAT.jsonl\",\n",
      "  \"fails_path\": \"/home/hello/Appeal_reader/indexes/EAT/failures__EAT.jsonl\",\n",
      "  \"cache_path\": \"/home/hello/Appeal_reader/indexes/EAT/cache__EAT.json\",\n",
      "  \"elapsed_s\": 0.52\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# =========================\n",
    "# SINGLE SELECTOR (HAMSTER SAFE)\n",
    "# =========================\n",
    "MODE = \"EAT\"  # \"EAT\" or \"ET\"\n",
    "if MODE not in {\"EAT\", \"ET\"}:\n",
    "    raise ValueError(\"MODE must be 'EAT' or 'ET'\")\n",
    "\n",
    "# =========================\n",
    "# HARD PATHS (EXPLICIT)\n",
    "# =========================\n",
    "PDF_DIRS = {\n",
    "    \"EAT\": Path(\"/media/hello/Tribunals/EAT_Appeals\").resolve(),\n",
    "    \"ET\":  Path(\"/media/hello/Tribunals/ET_Cases\").resolve(),\n",
    "}\n",
    "PDF_ROOT = PDF_DIRS[MODE]\n",
    "if not PDF_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"[{MODE}] PDF_ROOT does not exist: {PDF_ROOT}\")\n",
    "if not PDF_ROOT.is_dir():\n",
    "    raise NotADirectoryError(f\"[{MODE}] PDF_ROOT is not a directory: {PDF_ROOT}\")\n",
    "\n",
    "# =========================\n",
    "# OUTPUT DIR (MODE-SCOPED)\n",
    "# =========================\n",
    "BASE_OUT_DIR = Path(\"/home/hello/Appeal_reader/indexes\").resolve()\n",
    "\n",
    "OUT_DIR = (BASE_OUT_DIR / MODE).resolve()\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "CACHE_PATH = OUT_DIR / f\"cache__{MODE}.json\"\n",
    "LABELS_PATH = OUT_DIR / f\"labels__{MODE}.jsonl\"\n",
    "FAILS_PATH = OUT_DIR / f\"failures__{MODE}.jsonl\"\n",
    "STATS_PATH = OUT_DIR / f\"stats__{MODE}.json\"\n",
    "\n",
    "# =========================\n",
    "# TUNING\n",
    "# =========================\n",
    "FIRST_N_PAGES = 2            # sniff only first pages\n",
    "MAX_WORKERS = max(1, os.cpu_count() or 1)  # TR 24c -> go wide\n",
    "CHUNK_PRINT_EVERY = 2000     # progress prints\n",
    "\n",
    "# =========================\n",
    "# REGEX RULES (FAST TRIAGE)\n",
    "# =========================\n",
    "RULES: Dict[str, re.Pattern] = {\n",
    "    \"UNFAIR_DISMISSAL\": re.compile(r\"\\bunfair dismissal\\b|\\bera\\s*1996\\b|\\bsection\\s*98\\b|\\bs\\.?\\s*98\\b|\\bs\\.?\\s*98\\s*\\(\\s*4\\s*\\)\\b\", re.I),\n",
    "    \"CONSTRUCTIVE_DISMISSAL\": re.compile(r\"\\bconstructive dismissal\\b\", re.I),\n",
    "    \"WRONGFUL_DISMISSAL\": re.compile(r\"\\bwrongful dismissal\\b\", re.I),\n",
    "    \"WHISTLEBLOWING_PIDA\": re.compile(r\"\\bwhistleblow|\\bprotected disclosure\\b|\\bpida\\b|\\bpublic interest disclosure\\b\", re.I),\n",
    "    \"DISCRIMINATION\": re.compile(r\"\\bequality act\\b|\\beqa\\s*2010\\b|\\bdiscrimination\\b|\\bharass\\w*\\b|\\bvictimisa\\w*\\b\", re.I),\n",
    "    \"REDUNDANCY\": re.compile(r\"\\bredundan\\w*\\b\", re.I),\n",
    "    \"TUPE\": re.compile(r\"\\btupe\\b|\\btransfer of undertakings\\b\", re.I),\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def load_cache(path: Path) -> Dict[str, Any]:\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        obj = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "        return obj if isinstance(obj, dict) else {}\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def save_cache(path: Path, cache: Dict[str, Any]) -> None:\n",
    "    tmp = path.with_suffix(\".tmp\")\n",
    "    tmp.write_text(json.dumps(cache, ensure_ascii=False), encoding=\"utf-8\")\n",
    "    tmp.replace(path)\n",
    "\n",
    "def iter_pdfs(root: Path) -> List[Path]:\n",
    "    # deterministic ordering\n",
    "    return sorted(root.rglob(\"*.pdf\"))\n",
    "\n",
    "def file_sig(p: Path) -> Tuple[int, int]:\n",
    "    st = p.stat()\n",
    "    return (int(st.st_size), int(st.st_mtime))\n",
    "\n",
    "def classify_text(txt: str) -> Dict[str, bool]:\n",
    "    if not txt:\n",
    "        return {k: False for k in RULES}\n",
    "    return {k: bool(rx.search(txt)) for k, rx in RULES.items()}\n",
    "\n",
    "def extract_first_pages_text(pdf_path: str, first_n_pages: int) -> str:\n",
    "    \"\"\"\n",
    "    Uses PyMuPDF (fitz) because it's fast and reliable for text sniffing.\n",
    "    \"\"\"\n",
    "    import fitz  # type: ignore\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "    n = min(first_n_pages, doc.page_count)\n",
    "    parts = []\n",
    "    for i in range(n):\n",
    "        page = doc.load_page(i)\n",
    "        parts.append(page.get_text(\"text\") or \"\")\n",
    "    doc.close()\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "def worker(pdf_path: str, first_n_pages: int) -> Dict[str, Any]:\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        txt = extract_first_pages_text(pdf_path, first_n_pages)\n",
    "        labels = classify_text(txt)\n",
    "        out = {\n",
    "            \"path\": pdf_path,\n",
    "            \"labels\": labels,\n",
    "            \"text_ok\": True,\n",
    "            \"text_chars\": len(txt),\n",
    "            \"elapsed_s\": round(time.time() - t0, 4),\n",
    "        }\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"path\": pdf_path,\n",
    "            \"labels\": {k: False for k in RULES},\n",
    "            \"text_ok\": False,\n",
    "            \"error\": repr(e),\n",
    "            \"elapsed_s\": round(time.time() - t0, 4),\n",
    "        }\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "def main() -> None:\n",
    "    cache = load_cache(CACHE_PATH)\n",
    "\n",
    "    pdfs = iter_pdfs(PDF_ROOT)\n",
    "    total = len(pdfs)\n",
    "    print(f\"[{MODE}] PDF_ROOT={PDF_ROOT}\")\n",
    "    print(f\"[{MODE}] PDFs found: {total:,}\")\n",
    "    print(f\"[{MODE}] MAX_WORKERS={MAX_WORKERS} FIRST_N_PAGES={FIRST_N_PAGES}\")\n",
    "    print(f\"[{MODE}] OUT_DIR={OUT_DIR}\")\n",
    "\n",
    "    # Build task list (incremental)\n",
    "    tasks: List[Path] = []\n",
    "    reused = 0\n",
    "    for p in pdfs:\n",
    "        sig = file_sig(p)\n",
    "        key = str(p)\n",
    "        prev = cache.get(key)\n",
    "        if prev and prev.get(\"sig\") == list(sig):\n",
    "            reused += 1\n",
    "        else:\n",
    "            tasks.append(p)\n",
    "\n",
    "    print(f\"[{MODE}] Cache reused: {reused:,}\")\n",
    "    print(f\"[{MODE}] To process:   {len(tasks):,}\")\n",
    "\n",
    "    # Open outputs (append-friendly, but we will rewrite fresh for simplicity)\n",
    "    # We'll accumulate results then write JSONL once to avoid duplicates.\n",
    "    results: Dict[str, Any] = {}\n",
    "    failures: List[Dict[str, Any]] = []\n",
    "\n",
    "    # Pre-load cached results into results dict\n",
    "    for p in pdfs:\n",
    "        key = str(p)\n",
    "        prev = cache.get(key)\n",
    "        if prev and \"result\" in prev:\n",
    "            results[key] = prev[\"result\"]\n",
    "\n",
    "    processed = 0\n",
    "    t_start = time.time()\n",
    "\n",
    "    if tasks:\n",
    "        with ProcessPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "            futs = {ex.submit(worker, str(p), FIRST_N_PAGES): str(p) for p in tasks}\n",
    "            for fut in as_completed(futs):\n",
    "                path = futs[fut]\n",
    "                res = fut.result()\n",
    "                results[path] = res\n",
    "                cache[path] = {\"sig\": list(file_sig(Path(path))), \"result\": res}\n",
    "                processed += 1\n",
    "\n",
    "                if (processed % CHUNK_PRINT_EVERY) == 0:\n",
    "                    elapsed = time.time() - t_start\n",
    "                    print(f\"[{MODE}] processed {processed:,}/{len(tasks):,} new in {elapsed:.1f}s\")\n",
    "\n",
    "    # Write JSONL fresh (deduped, deterministic order)\n",
    "    ok_count = 0\n",
    "    hit_counts = {k: 0 for k in RULES}\n",
    "    text_fail = 0\n",
    "\n",
    "    with LABELS_PATH.open(\"w\", encoding=\"utf-8\") as f_out, FAILS_PATH.open(\"w\", encoding=\"utf-8\") as f_fail:\n",
    "        for p in pdfs:\n",
    "            key = str(p)\n",
    "            res = results.get(key)\n",
    "            if not res:\n",
    "                # Shouldn't happen, but guard\n",
    "                continue\n",
    "\n",
    "            if not res.get(\"text_ok\"):\n",
    "                text_fail += 1\n",
    "                f_fail.write(json.dumps(res, ensure_ascii=False) + \"\\n\")\n",
    "            else:\n",
    "                ok_count += 1\n",
    "\n",
    "            labels = res.get(\"labels\") or {}\n",
    "            for k in RULES:\n",
    "                if labels.get(k):\n",
    "                    hit_counts[k] += 1\n",
    "\n",
    "            f_out.write(json.dumps(res, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # Save cache + stats\n",
    "    save_cache(CACHE_PATH, cache)\n",
    "\n",
    "    stats = {\n",
    "        \"mode\": MODE,\n",
    "        \"pdf_root\": str(PDF_ROOT),\n",
    "        \"total_pdfs\": total,\n",
    "        \"text_ok\": ok_count,\n",
    "        \"text_fail\": text_fail,\n",
    "        \"hits\": hit_counts,\n",
    "        \"first_n_pages\": FIRST_N_PAGES,\n",
    "        \"max_workers\": MAX_WORKERS,\n",
    "        \"labels_path\": str(LABELS_PATH),\n",
    "        \"fails_path\": str(FAILS_PATH),\n",
    "        \"cache_path\": str(CACHE_PATH),\n",
    "        \"elapsed_s\": round(time.time() - t_start, 2),\n",
    "    }\n",
    "    STATS_PATH.write_text(json.dumps(stats, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"[{MODE}] DONE.\")\n",
    "    print(json.dumps(stats, ensure_ascii=False, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7893c1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>path</th><th>text_ok</th><th>text_chars</th><th>elapsed_s</th><th>UNFAIR_DISMISSAL</th><th>CONSTRUCTIVE_DISMISSAL</th><th>WRONGFUL_DISMISSAL</th><th>WHISTLEBLOWING_PIDA</th><th>DISCRIMINATION</th><th>REDUNDANCY</th><th>TUPE</th></tr><tr><td>str</td><td>bool</td><td>i64</td><td>f64</td><td>bool</td><td>bool</td><td>bool</td><td>bool</td><td>bool</td><td>bool</td><td>bool</td></tr></thead><tbody><tr><td>&quot;/media/hello/Tribunals/EAT_App…</td><td>true</td><td>2749</td><td>0.1603</td><td>true</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td></tr><tr><td>&quot;/media/hello/Tribunals/EAT_App…</td><td>true</td><td>3220</td><td>0.1379</td><td>true</td><td>false</td><td>false</td><td>true</td><td>false</td><td>false</td><td>false</td></tr><tr><td>&quot;/media/hello/Tribunals/EAT_App…</td><td>true</td><td>994</td><td>0.1234</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td></tr><tr><td>&quot;/media/hello/Tribunals/EAT_App…</td><td>true</td><td>728</td><td>0.1229</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td></tr><tr><td>&quot;/media/hello/Tribunals/EAT_App…</td><td>true</td><td>1035</td><td>0.1347</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 11)\n",
       "┌────────────┬─────────┬────────────┬───────────┬───┬────────────┬────────────┬────────────┬───────┐\n",
       "│ path       ┆ text_ok ┆ text_chars ┆ elapsed_s ┆ … ┆ WHISTLEBLO ┆ DISCRIMINA ┆ REDUNDANCY ┆ TUPE  │\n",
       "│ ---        ┆ ---     ┆ ---        ┆ ---       ┆   ┆ WING_PIDA  ┆ TION       ┆ ---        ┆ ---   │\n",
       "│ str        ┆ bool    ┆ i64        ┆ f64       ┆   ┆ ---        ┆ ---        ┆ bool       ┆ bool  │\n",
       "│            ┆         ┆            ┆           ┆   ┆ bool       ┆ bool       ┆            ┆       │\n",
       "╞════════════╪═════════╪════════════╪═══════════╪═══╪════════════╪════════════╪════════════╪═══════╡\n",
       "│ /media/hel ┆ true    ┆ 2749       ┆ 0.1603    ┆ … ┆ false      ┆ false      ┆ false      ┆ false │\n",
       "│ lo/Tribuna ┆         ┆            ┆           ┆   ┆            ┆            ┆            ┆       │\n",
       "│ ls/EAT_App ┆         ┆            ┆           ┆   ┆            ┆            ┆            ┆       │\n",
       "│ …          ┆         ┆            ┆           ┆   ┆            ┆            ┆            ┆       │\n",
       "│ /media/hel ┆ true    ┆ 3220       ┆ 0.1379    ┆ … ┆ true       ┆ false      ┆ false      ┆ false │\n",
       "│ lo/Tribuna ┆         ┆            ┆           ┆   ┆            ┆            ┆            ┆       │\n",
       "│ ls/EAT_App ┆         ┆            ┆           ┆   ┆            ┆            ┆            ┆       │\n",
       "│ …          ┆         ┆            ┆           ┆   ┆            ┆            ┆            ┆       │\n",
       "│ /media/hel ┆ true    ┆ 994        ┆ 0.1234    ┆ … ┆ false      ┆ false      ┆ false      ┆ false │\n",
       "│ lo/Tribuna ┆         ┆            ┆           ┆   ┆            ┆            ┆            ┆       │\n",
       "│ ls/EAT_App ┆         ┆            ┆           ┆   ┆            ┆            ┆            ┆       │\n",
       "│ …          ┆         ┆            ┆           ┆   ┆            ┆            ┆            ┆       │\n",
       "│ /media/hel ┆ true    ┆ 728        ┆ 0.1229    ┆ … ┆ false      ┆ false      ┆ false      ┆ false │\n",
       "│ lo/Tribuna ┆         ┆            ┆           ┆   ┆            ┆            ┆            ┆       │\n",
       "│ ls/EAT_App ┆         ┆            ┆           ┆   ┆            ┆            ┆            ┆       │\n",
       "│ …          ┆         ┆            ┆           ┆   ┆            ┆            ┆            ┆       │\n",
       "│ /media/hel ┆ true    ┆ 1035       ┆ 0.1347    ┆ … ┆ false      ┆ false      ┆ false      ┆ false │\n",
       "│ lo/Tribuna ┆         ┆            ┆           ┆   ┆            ┆            ┆            ┆       │\n",
       "│ ls/EAT_App ┆         ┆            ┆           ┆   ┆            ┆            ┆            ┆       │\n",
       "│ …          ┆         ┆            ┆           ┆   ┆            ┆            ┆            ┆       │\n",
       "└────────────┴─────────┴────────────┴───────────┴───┴────────────┴────────────┴────────────┴───────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "path = \"/home/hello/Appeal_reader/indexes/EAT/labels__EAT.jsonl\"\n",
    "\n",
    "df = pl.read_ndjson(path)\n",
    "\n",
    "label_cols = df.select(\"labels\").schema[\"labels\"].fields  # list of StructField\n",
    "names = [f.name for f in label_cols]\n",
    "\n",
    "df = df.with_columns([pl.col(\"labels\").struct.field(n).alias(n) for n in names]).drop(\"labels\")\n",
    "\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
